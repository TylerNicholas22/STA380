---
title: "HW2_Tyler_Nicholas.Rmd"
author: "Tyler Nicholas"
date: "August 9, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


## **Flights at ABIA**


We are trying to answer the question of which month is the best time to fly to minimize delays.  For the purpose of this question we define delays to be leaving over 30 minutes after scheduled departure time. The data was masked to show only flights leaving Austin so we could determine the best time to fly out of Austin.  

In Figure 1 we see the number of delays that are longer than half an hour each month.  There are a distinct top 3 and a distinct bottom 3.  Here we see that June, December, and March are the months with the most flights delayed over half an hour and November, October, and September are the months with the least.  This gives us an idea of the best months to fly to reduce massive delays.  However this may not be telling us the full picture.  We really want to see what percentage of flights are delayed each month to see what the likelihood is of having our flight delayed.  We see this in Figure 2.  We get the same top 3 and bottom 3 months.

With this we can say that November, October, and September are the best times to fly out of Austin to reduce delays.  If you are flying out of Austin in June, December, or March your flight is likely to be delayed.  This means that if you have a connecting flight, you should schedule a longer layover to make sure that you catch your next flight.


```{r, echo = FALSE}

flight_data <- read.csv("ABIA.csv")
austin_only <- read.csv("ABIA_AUS.csv")


no_outliers_only = flight_data[flight_data$DepDelay < 100,]
delays_only = austin_only[austin_only$DepDelay > 30,]
delays_over_30 = austin_only[austin_only$DepDelay > 30,]
over_30_counts = table(delays_over_30$Month)

counts <- table(delays_only$Month)
counts2 <- table(austin_only$Month)
daycounts <- table(delays_only$DayOfWeek)
count_ratio <- counts/counts2

#barplot(counts)
#barplot(counts2)
#barplot(daycounts)

over_30_counts <- sort(over_30_counts, decreasing = TRUE)


barplot(over_30_counts, ylim = c(0,600), cex.names = 0.7, main = "Figure 1: \n Number of Delays", names.arg = c("Jun","Dec", "Mar", "Jul", "Aug", "Feb", "May", "Apr", "Jan", "Nov", "Oct", "Sep"), col = heat.colors(12))
count_ratio <- sort(count_ratio, decreasing = TRUE )


barplot(count_ratio, ylim=c(0,.15), cex.names = 0.7, names.arg = c("Dec", "Mar", "Jun", "Feb", "Aug", "May", "Jul", "Jan", "Apr", "Oct", "Nov", "Sep"), main = "Figure 2: \n Percentage of Flights Delayed Each Month", col = heat.colors(12))


#aggregate(DepDelay~Month, austin_only, mean)
#aggregate(DepDelay~Month, austin_only, median)



```


## **Author Attribution**

```{r, echo = FALSE, include = FALSE}
library(nnet)
library(tm)
## tm has many "reader" functions.  Each one has
## arguments elem, language, id (see ?readPlain,?readPDF,etc)
## This wraps another function around readPlan to read
## plain text documents in English.
readerPlain = function(fname){readPlain(elem=list(content=readLines(fname)),id=fname, language='en') }



author_dirs = Sys.glob('ReutersC50/C50train/*')
author_dirs = author_dirs[1:50]
file_list = NULL
labels = NULL
for(author in author_dirs) {
	author_name = substring(author, first=21)
	author_name
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels = append(labels, rep(author_name, length(files_to_add)))
}
author_dirs_test = Sys.glob('ReutersC50/C50test/*')
author_dirs_test = author_dirs_test[1:50]
file_list_test = NULL
labels_test = NULL
for(author in author_dirs_test) {
	author_name = substring(author, first=20)
	author_name
	files_to_add_test = Sys.glob(paste0(author, '/*.txt'))
	file_list_test = append(file_list_test, files_to_add_test)
	labels_test = append(labels, rep(author_name, length(files_to_add_test)))
}
i = 1
y = c()
y_num = c()
for(author in author_dirs) {
  author_name = substring(author, first=21)
  for(k in c(1:50)){
    y = append(y,author_name)
    y_num = append(y_num, i)
  }
  i = i+1
  
}  
	simon = lapply(file_list, readerPlain) 


## once you have documents in a vector, you 
## create a text mining 'corpus' with: 
my_documents = Corpus(VectorSource(simon))
names(my_documents) = names(simon) # come on, tm! this should just happen.


## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space

## Remove stopwords.  Always be careful with this: one man's trash is another one's treasure.
stopwords("en")
stopwords("SMART")
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))


## create a doc-term-matrix
DTM_simon = DocumentTermMatrix(my_documents)
DTM_simon # some basic summary statistics

class(DTM_simon)  # a special kind of sparse matrix format

## You can inspect its entries...
inspect(DTM_simon[1:10,1:20])

## ...find words with greater than a min count...
findFreqTerms(DTM_simon, 50)

## ...or find words whose count correlates with a specified word.
findAssocs(DTM_simon, "market", .5) 

## Finally, drop those terms that only occur in one or two documents
## This is a common step: the noise of the "long tail" (rare terms)
##	can be huge, and there is nothing to learn if a term occured once.
## Below removes those terms that have count 0 in >95% of docs.  
## Probably a bit stringent here... but only 50 docs!
DTM_simon = removeSparseTerms(DTM_simon, 0.95)
DTM_simon # now ~ 1000 terms (versus ~3000 before)

# Now PCA on term frequencies
X = as.matrix(DTM_simon)
X = X/rowSums(X)  # term-frequency weighting

pca_simon = prcomp(X, scale=TRUE)
plot(pca_simon) 

# Look at the loadings
pca_simon$rotation[order(abs(pca_simon$rotation[,1]),decreasing=TRUE),1][1:25]
pca_simon$rotation[order(abs(pca_simon$rotation[,2]),decreasing=TRUE),2][1:25]


## Plot the first two PCs..
plot(pca_simon$x[,1:2], xlab="PCA 1 direction", ylab="PCA 2 direction",bty="n",type='n')
text(pca_simon$x[,1:2], labels = 1:length(simon), cex=0.7)
#identify(pca_simon$x[,1:2], n=4)

# Both about "Scottish Amicable"
content(simon[[46]])
content(simon[[48]])

# Both about genetic testing
content(simon[[25]])
content(simon[[26]])

# Both about Ladbroke's merger
content(simon[[10]])
content(simon[[11]])


#run regression
K = 782
V = pca_simon$rotation[,1:K]
X
scores = X %*% V


#pcr1 = multinom(y_num ~ scores, maxit = 1000)
#pcr1$fitted.values
#X_dataframe = as.data.frame(X)

#prediction = predict(pcr1,)
#tail(y_num)
#tail(prediction)
#plot(fitted(pcr1), y_num)
#fitted(pcr1)

```



```{r, echo = FALSE, include = FALSE}

#NaiveBayes
simon_test = lapply(file_list_test, readerPlain) 


## once you have documents in a vector, you 
## create a text mining 'corpus' with: 
my_documents_test = Corpus(VectorSource(simon_test))
names(my_documents_test) = names(simon_test) # come on, tm! this should just happen.


## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents_test = tm_map(my_documents_test, content_transformer(tolower)) # make everything lowercase
my_documents_test = tm_map(my_documents_test, content_transformer(removeNumbers)) # remove numbers
my_documents_test = tm_map(my_documents_test, content_transformer(removePunctuation)) # remove punctuation
my_documents_test = tm_map(my_documents_test, content_transformer(stripWhitespace)) ## remove excess white-space

## Remove stopwords.  Always be careful with this: one man's trash is another one's treasure.

my_documents_test = tm_map(my_documents_test, content_transformer(removeWords), stopwords("SMART"))
?stopwords

## create a doc-term-matrix
DTM_simon_test = DocumentTermMatrix(my_documents_test)
DTM_simon_test # some basic summary statistics

DTM_simon_test = removeSparseTerms(DTM_simon_test, 0.95)

X_test = as.matrix(DTM_simon)
X_test = X_test/rowSums(X_test)


smooth_count = 1/nrow(X)
k =1
w_list = c()
for(i in c(1:50)){
  nam <- paste("train_",i,sep = "")
  assign(nam,X[k:(k+49),])
  nam
  nam2 <- paste("w_",i,sep = "")
  assign(nam2,(colSums(X[k:(k+49),]+smooth_count))/sum(colSums(X[k:(k+49),]+smooth_count)))
  k=k+50
}



W_list = data.frame(w_1,w_2,w_3,w_4,w_5,w_6,w_7,w_8,w_9,w_10,w_11, w_12, w_13, w_14,w_15,w_16,w_17,w_18, w_19, w_20, w_21, w_22, w_23, w_24, w_25, w_26, w_27, w_28, w_29, w_30, w_31, w_32, w_33, w_34, w_35, w_36, w_37, w_38, w_39, w_40, w_41, w_42, w_43, w_44, w_45, w_46, w_47, w_48, w_49, w_50)

i=0
k=0
predict_list = c()
for(k in c(1:2500)){
  sums_list = c()
  for(i in c(1:50)){
    sums = sum(X_test[k,]*log(W_list[,i]))
    sums_list = append(sums_list, sums)
    
  }
  predict_list = append(predict_list,which.max(sums_list))
}

i=0
count =0
wrong_list = c()
wrong_id = c()
for(i in c(1:2500)){
  if(predict_list[i]==y_num[i]){
    count = count +1
  }
  else{
    wrong_id = append(wrong_id,i)
    wrong_list = append(wrong_list,predict_list[i])
  }
} 
predict_list
count/2500

```


We made two different models, one using multinomial regression and the other using Naive Bayes to predict the author of the test set documents.  The regression model only gave us 55.28 % accuracy on the test set so we will focus most of our discussion on the Naive Bayes model which gave us an accuracy of 91.16% on the test set.  
```{r, echo = FALSE}
count/2500
```

Our Naive Bayes model was run with the 782 components of the PCA and was by far the better of the two models we created.  It was also preferable since it took much less time to run.  In this particular scenario it was preferable in performance and runtime.  

Since the Naive Bayes was our preferred model, lets look at some of the authors that we had difficulty distinguishing from one another in the Naive Bayes Model:   


Edna Fernandes: Tim Farrand   
Eric Auchard  : Therese Poletti   
Joe Ortiz     : Tim Farrand   
Martin Wolk   : Therese Poletti   
Sarah Davison : Peter Humphrey   
Todd Nissen   : David Lawder  

The left hand side is the actual writer and the right hand side is the writer that we incorrectly predicted.  We are only showing instances that occurred at least 4 times in our data set.  With 91% accuracy, 4 incorrect instances of the same writers is notable.  We can see that there are multiple writers that are repeatedly confused with both Tim Farrand and Therese Poletti.

Though there were a few instances where the Naive Bayes model was incorrect, it was more accurate and quicker to run than our regression model and is overall the best model we created.



```{r, include = FALSE}
#Regression
library(tm)
library(nnet)
library(klaR)
source('textutils.R')
author_dirs_2 = Sys.glob('ReutersC50/C50train/*') # train
author_dirs_test = Sys.glob('ReutersC50/C50test/*') # test

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)),
            id=fname, language='en') }

## Import and process training set
file_list_2 = NULL
labels_2 = NULL

for(author in author_dirs_2) {
  author_name = substring(author, first=29)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_2 = append(file_list_2, files_to_add)
  labels_2 = append(labels, rep(author_name, length(files_to_add)))
}

all_docs_2 = lapply(file_list_2, readerPlain)
names(all_docs_2) = file_list_2
names(all_docs_2) = sub('.txt', '', names(all_docs_2))

my_corpus_2 = Corpus(VectorSource(all_docs_2))
names(my_corpus_2) = file_list_2

my_corpus_2 = tm_map(my_corpus_2, content_transformer(tolower)) # make everything lowercase
my_corpus_2 = tm_map(my_corpus_2, content_transformer(removeNumbers)) # remove numbers
my_corpus_2 = tm_map(my_corpus_2, content_transformer(removePunctuation)) # remove punctuation
my_corpus_2 = tm_map(my_corpus_2, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_2 = tm_map(my_corpus_2, content_transformer(removeWords), stopwords("SMART"))

DTM_2 = DocumentTermMatrix(my_corpus_2)

X2 = as.matrix(DTM_2)

## Import and process test set
file_list_test = NULL
labels_test = NULL

for(author in author_dirs_test) {
  author_name = substring(author, first=29)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels_test = append(labels, rep(author_name, length(files_to_add)))
}

all_docs_test = lapply(file_list_test, readerPlain)
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))

my_corpus_test = Corpus(VectorSource(all_docs_test))
names(my_corpus_test) = file_list_test

my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))

DTM_test = DocumentTermMatrix(my_corpus_test)

X_test = as.matrix(DTM_test)


## Create training set for PCA logistic regression
train_log = as.data.frame(X2)
train_log$target = 0
train_log$new_word = 0

test_log = as.data.frame(X_test)
test_log$target = 0


## Create target variables that correspond to each author
for (i in 1:50) {
  start_idx = 50*(i-1) + 1
  end_idx = (50*i)

  train_log$target[start_idx:end_idx] = i
  test_log$target[start_idx:end_idx] = i
}

train_log$target = factor(train_log$target)
test_log$target = factor(test_log$target)

## Create X and Y training sets
X.log = as.matrix(train_log[,names(train_log)!='target'])
y.log = train_log$target

## Create X and Y test sets
X.test = as.matrix(test_log[,names(test_log)!='target'])
y.test = test_log$target

test.matrix = matrix(data=0,nrow=dim(X.log)[1],ncol=dim(X.log)[2]) # test matrix with same columns as train
colnames(test.matrix) = colnames(X.log)

# Build function to sapply over test matrix
match_columns = which(colnames(X.test) %in% colnames(test.matrix))
match_train = which(colnames(test.matrix) %in% colnames(X.test[,match_columns]))

test.matrix[,match_train] = X.test[,match_columns]


## Run PCA
pc_author = prcomp(X.log, center =TRUE)
K = 100
V = pc_author$rotation[,1:K]
S = X.log %*% V # S-Matrix

## Run Multinomial Logistic Regression
model_multi = multinom(y.log ~ .,data=as.data.frame(S),MaxNWts=10200)

## Training predictions
predict_log = predict(model_multi,as.matrix(test.matrix)%*%V)

length(predict_log[predict_log[1:10] == y.test[1:10]])/10 # Accuracy

get_accuracy = function(k){
  trues=0
  for (i in 1:k){
    if (predict_log[i] == y.test[i]) {
      trues=trues+1
    } else {
      trues=trues
    }
  }
  trues/k
}
get_accuracy(2500)



```

## **Practice with association rule mining**

```{r,message = FALSE, echo = FALSE, results = 'hide'}
library(arules)
library(reshape2)

groceries_raw <- read.csv("grocery.csv")

groceries_raw_t <- t(groceries_raw)
groceries_raw_melt <- melt(groceries_raw_t, measure.vars = 'ID')



new_groceries = groceries_raw_melt[groceries_raw_melt$value != "",]
new_groceries = new_groceries[new_groceries$Var1 != "ID",]


new_groceries$Var2 <- factor(new_groceries$Var2)


groceries <- split(x=new_groceries$value, f=new_groceries$Var2)

## Remove duplicates ("de-dupe")
groceries <- lapply(groceries, unique)

groctrans <- as(groceries, "transactions")

groceryrules <- apriori(groctrans, parameter=list(support=.005, confidence=.5, maxlen=20, minlen =1))

# Look at the output
#inspect(groceryrules)
```


We take the grocery baskets for over 9000 customers to examine what relationships exist between items in those baskets.  We pick thresholds for support, confidence and lift.  Here we chose .005 for support to get item combinations that appear fairly often in baskets.  Then we did a confidence threshold of .5 so that we only predict items that are in half of the baskets containing the left hand side items.  Lastly we use a lift threshold of 3.2 to only get predicted relationships that make you much more likely to by the item on the right hand side. These thresholds were chosen to find unique relationships with very high lift.  We can see the two resulting relationships here:


```{r, echo = FALSE}
## Choose a subset
inspect(subset(groceryrules, subset=lift > 3.2))
#inspect(subset(groceryrules, subset=confidence > 0.55))
#inspect(subset(groceryrules, subset=support > .01 & confidence > 0.55))

```

 
 From this we see that if you have curd and tropical fruit in your basket, you are 3.69 times more likely to buy yogurt than the average person.  Similarly if you have citrus fruit, root vegetables, and whole milk in your basket, you are 3.27 times more likely to buy other vegetables.  These discoveries can lead us to create coupons encouraging purchasing tropical fruit and curd in order to bolster yogurt sales or similarly giving coupons for citrus fruit, root vegetables, and whole milk to encourage purchasing of other vegetables.
